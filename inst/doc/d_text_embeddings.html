<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Thomas W. Jones" />

<meta name="date" content="2018-10-31" />

<title></title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore"><ol start="4" style="list-style-type: decimal">
<li>Text embeddings</li>
</ol></h1>
<h4 class="author"><em>Thomas W. Jones</em></h4>
<h4 class="date"><em>2018-10-31</em></h4>



<div id="text-embeddings" class="section level1">
<h1>Text embeddings</h1>
<p><a href="https://en.wikipedia.org/wiki/Word_embedding">Text embeddings</a> are particularly hot right now. While textmineR doesn’t (yet) explicitly implement any embedding models like GloVe or word2vec, you can still get embeddings. Text embedding algorithms aren’t conceptually different from topic models. They are, however, operating on a different matrix. Instead of reducing the dimensions of a document term matrix, text embeddings are obtained by reducing the dimensions of a term co-occurrence matrix. In principle, one can use LDA or LSA in the same way. In this case, rows of theta are embedded words. A phi_prime may be obtained to project documents or new text into the embedding space.</p>
<div id="create-a-term-co-occurrence-matrix" class="section level2">
<h2>Create a term co-occurrence matrix</h2>
<p>The first step in fitting a text embedding model is to <a href="https://stackoverflow.com/questions/24073030/what-are-co-occurance-matrixes-and-how-are-they-used-in-nlp">create a term co-occurrence matrix</a> or TCM. In a TCM, both columns and rows index tokens. The <span class="math inline">\((i,j)\)</span> entries of the matrix are a count of the number of times word <span class="math inline">\(i\)</span> co-occurs with <span class="math inline">\(j\)</span>. However, there are several ways to count co-occurrence. textmineR gives you three.</p>
<p>The most useful way of counting co-occurrence for text embeddings is called the skip-gram model. Under the skip-gram model, the count would be the number of times word <span class="math inline">\(j\)</span> appears within a certain window of <span class="math inline">\(i\)</span>. A skip-gram window of two, for example, would count the number of times word <span class="math inline">\(j\)</span> occurred in the two words immediately before word <span class="math inline">\(i\)</span> or the two words immediately after word <span class="math inline">\(i\)</span>. This helps capture the local context of words. In fact, you can think of a text embedding as being a topic model based on the local context of words. Whereas a traditional topic model is modeling words in their global context.</p>
<p>To read more about the skip-gram model, which was popularized in the embedding model word2vec, look <a href="https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4">here</a>.</p>
<p>The other types of co-occurrence matrix textmineR provides are both global. One is a count of the number of documents in which words <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> co-occur. The other is the number of terms that co-occur between <em>documents</em> <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. See <code>help(CreateTcm)</code> for info on these.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># load the NIH data set</span>
<span class="kw">library</span>(textmineR)

<span class="co"># load nih_sample data set from textmineR</span>
<span class="kw">data</span>(nih_sample)

<span class="co"># First create a TCM using skip grams, we'll use a 5-word window</span>
<span class="co"># most options available on CreateDtm are also available for CreateTcm</span>
tcm &lt;-<span class="st"> </span><span class="kw">CreateTcm</span>(<span class="dt">doc_vec =</span> nih_sample<span class="op">$</span>ABSTRACT_TEXT,
                 <span class="dt">skipgram_window =</span> <span class="dv">10</span>,
                 <span class="dt">verbose =</span> <span class="ot">FALSE</span>,
                 <span class="dt">cpus =</span> <span class="dv">2</span>)

<span class="co"># a TCM is generally larger than a DTM</span>
<span class="kw">dim</span>(tcm)
<span class="co">#&gt; [1] 5210 5210</span></code></pre></div>
</div>
<div id="fitting-a-model" class="section level2">
<h2>Fitting a model</h2>
<p>Once we have a TCM, we can use the same procedure to make an embedding model as we used to make a topic model. Note that it may take considerably longer (because of dimensionality of the matrix) or shorter (because of sparsity) to fit an embedding on the same corpus.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># use LDA to get embeddings into probability space</span>
<span class="co"># This will take considerably longer as the TCM matrix has many more rows </span>
<span class="co"># than your average DTM</span>
embeddings &lt;-<span class="st"> </span><span class="kw">FitLdaModel</span>(<span class="dt">dtm =</span> tcm,
                          <span class="dt">k =</span> <span class="dv">50</span>,
                          <span class="dt">iterations =</span> <span class="dv">200</span>,
                          <span class="dt">burnin =</span> <span class="dv">180</span>,
                          <span class="dt">alpha =</span> <span class="fl">0.1</span>,
                          <span class="dt">beta =</span> <span class="fl">0.05</span>,
                          <span class="dt">optimize_alpha =</span> <span class="ot">TRUE</span>,
                          <span class="dt">calc_likelihood =</span> <span class="ot">FALSE</span>,
                          <span class="dt">calc_coherence =</span> <span class="ot">TRUE</span>,
                          <span class="dt">calc_r2 =</span> <span class="ot">TRUE</span>,
                          <span class="dt">cpus =</span> <span class="dv">2</span>)</code></pre></div>
</div>
<div id="interpretation-of-phi-and-theta" class="section level2">
<h2>Interpretation of <span class="math inline">\(\Phi\)</span> and <span class="math inline">\(\Theta\)</span></h2>
<p>In the language of text embeddings, <span class="math inline">\(\Theta\)</span> gives us our tokens embedded in a probability space (because we used LDA, Euclidean space if we used LSA). <span class="math inline">\(\Phi\)</span> defines the dimensions of our embedding space. The rows of <span class="math inline">\(\Phi\)</span> can still be interpreted as topics. But they are topics of local contexts, rather than within whole documents.</p>
</div>
<div id="evaluating-the-model" class="section level2">
<h2>Evaluating the model</h2>
<p>As it happens, the same evaluation metrics developed for topic modeling also apply here. There are subtle differences in interpretation because we are using a TCM not a DTM. i.e. occurrences relate words to each other, not to documents.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get an R-squared for general goodness of fit</span>
embeddings<span class="op">$</span>r2
<span class="co">#&gt; [1] 0.1746546</span>

<span class="co"># Get coherence (relative to the TCM) for goodness of fit</span>
<span class="kw">summary</span>(embeddings<span class="op">$</span>coherence)
<span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span>
<span class="co">#&gt;  0.0119  0.0731  0.1121  0.1280  0.1750  0.3218</span></code></pre></div>
<p>We will create a summary table as we did with a topic model before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get top terms, no labels because we don't have bigrams</span>
embeddings<span class="op">$</span>top_terms &lt;-<span class="st"> </span><span class="kw">GetTopTerms</span>(<span class="dt">phi =</span> embeddings<span class="op">$</span>phi,
                                    <span class="dt">M =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a summary table, similar to the above</span>
embeddings<span class="op">$</span>summary &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">topic =</span> <span class="kw">rownames</span>(embeddings<span class="op">$</span>phi),
                                 <span class="dt">coherence =</span> <span class="kw">round</span>(embeddings<span class="op">$</span>coherence, <span class="dv">3</span>),
                                 <span class="dt">prevalence =</span> <span class="kw">round</span>(<span class="kw">colSums</span>(embeddings<span class="op">$</span>theta), <span class="dv">2</span>),
                                 <span class="dt">top_terms =</span> <span class="kw">apply</span>(embeddings<span class="op">$</span>top_terms, <span class="dv">2</span>, <span class="cf">function</span>(x){
                                   <span class="kw">paste</span>(x, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>)
                                 }),
                                 <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>Here it is ordered by prevalence. (Here, we might say density of tokens along each embedding dimension.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">embeddings<span class="op">$</span>summary[ <span class="kw">order</span>(embeddings<span class="op">$</span>summary<span class="op">$</span>prevalence, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>) , ][ <span class="dv">1</span><span class="op">:</span><span class="dv">10</span> , ]</code></pre></div>
<table>
<caption>Summary of top 10 embedding dimensions</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">topic</th>
<th align="right">coherence</th>
<th align="right">prevalence</th>
<th align="left">top_terms</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>t_22</td>
<td align="left">t_22</td>
<td align="right">0.218</td>
<td align="right">184.11</td>
<td align="left">aim, specific, study, determine, studies</td>
</tr>
<tr class="even">
<td>t_41</td>
<td align="left">t_41</td>
<td align="right">0.178</td>
<td align="right">158.86</td>
<td align="left">research, health, cancer, clinical, development</td>
</tr>
<tr class="odd">
<td>t_21</td>
<td align="left">t_21</td>
<td align="right">0.209</td>
<td align="right">146.48</td>
<td align="left">core, research, program, support, training</td>
</tr>
<tr class="even">
<td>t_45</td>
<td align="left">t_45</td>
<td align="right">0.181</td>
<td align="right">123.89</td>
<td align="left">cells, cell, human, function, signaling</td>
</tr>
<tr class="odd">
<td>t_17</td>
<td align="left">t_17</td>
<td align="right">0.135</td>
<td align="right">118.94</td>
<td align="left">based, treatment, design, effect, development</td>
</tr>
<tr class="even">
<td>t_14</td>
<td align="left">t_14</td>
<td align="right">0.136</td>
<td align="right">113.22</td>
<td align="left">factors, risk, early, social, sud</td>
</tr>
<tr class="odd">
<td>t_16</td>
<td align="left">t_16</td>
<td align="right">0.149</td>
<td align="right">110.60</td>
<td align="left">role, mechanisms, gene, critical, genetic</td>
</tr>
<tr class="even">
<td>t_43</td>
<td align="left">t_43</td>
<td align="right">0.195</td>
<td align="right">110.52</td>
<td align="left">brain, cancer, tumor, cells, patients</td>
</tr>
<tr class="odd">
<td>t_49</td>
<td align="left">t_49</td>
<td align="right">0.144</td>
<td align="right">109.38</td>
<td align="left">response, tissue, responses, immune, host</td>
</tr>
<tr class="even">
<td>t_19</td>
<td align="left">t_19</td>
<td align="right">0.062</td>
<td align="right">107.02</td>
<td align="left">high, imaging, metastases, vivo, modified</td>
</tr>
</tbody>
</table>
<p>And here is the table ordered by coherence.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">embeddings<span class="op">$</span>summary[ <span class="kw">order</span>(embeddings<span class="op">$</span>summary<span class="op">$</span>coherence, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>) , ][ <span class="dv">1</span><span class="op">:</span><span class="dv">10</span> , ]</code></pre></div>
<table>
<caption>Summary of 10 most coherent embedding dimensions</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">topic</th>
<th align="right">coherence</th>
<th align="right">prevalence</th>
<th align="left">top_terms</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>t_31</td>
<td align="left">t_31</td>
<td align="right">0.322</td>
<td align="right">95.70</td>
<td align="left">provided, applicant, project, patients, description</td>
</tr>
<tr class="even">
<td>t_46</td>
<td align="left">t_46</td>
<td align="right">0.283</td>
<td align="right">101.39</td>
<td align="left">proteins, fly, cutaneous, infection, response</td>
</tr>
<tr class="odd">
<td>t_34</td>
<td align="left">t_34</td>
<td align="right">0.278</td>
<td align="right">98.03</td>
<td align="left">microbiome, gut, composition, crc, bas</td>
</tr>
<tr class="even">
<td>t_25</td>
<td align="left">t_25</td>
<td align="right">0.238</td>
<td align="right">97.01</td>
<td align="left">carbon, pathways, environment, intracellular, metabolism</td>
</tr>
<tr class="odd">
<td>t_5</td>
<td align="left">t_5</td>
<td align="right">0.221</td>
<td align="right">101.74</td>
<td align="left">fertility, ethnic, race, knowledge, force</td>
</tr>
<tr class="even">
<td>t_22</td>
<td align="left">t_22</td>
<td align="right">0.218</td>
<td align="right">184.11</td>
<td align="left">aim, specific, study, determine, studies</td>
</tr>
<tr class="odd">
<td>t_38</td>
<td align="left">t_38</td>
<td align="right">0.210</td>
<td align="right">99.11</td>
<td align="left">sleep, dependent, memory, cognitive, reduced</td>
</tr>
<tr class="even">
<td>t_21</td>
<td align="left">t_21</td>
<td align="right">0.209</td>
<td align="right">146.48</td>
<td align="left">core, research, program, support, training</td>
</tr>
<tr class="odd">
<td>t_15</td>
<td align="left">t_15</td>
<td align="right">0.203</td>
<td align="right">100.98</td>
<td align="left">lung, expression, ipf, patients, key</td>
</tr>
<tr class="even">
<td>t_50</td>
<td align="left">t_50</td>
<td align="right">0.200</td>
<td align="right">104.31</td>
<td align="left">ri, ptc, fc, pathways, brafv</td>
</tr>
</tbody>
</table>
</div>
<div id="embedding-documents-under-the-model" class="section level2">
<h2>Embedding documents under the model</h2>
<p>You can embed whole documents under your model. Doing so, effectively makes your embeddings a topic model that have topics of local contexts, instead of global ones. Why might you want to do this? The short answer is that you may have reason to believe that an embedding model may give you better topics, especially if you are trying to pick up on more subtle topics. In a later example, we’ll be doing that to build a document summarizer.</p>
<p>A note on the below: TCMs may be very sparse and cause us to run into computational underflow issues when using the “gibbs” prediction method. As a result, I’m choosing to use the “dot” method.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make a DTM from our documents</span>
dtm_embed &lt;-<span class="st"> </span><span class="kw">CreateDtm</span>(<span class="dt">doc_vec =</span> nih_sample<span class="op">$</span>ABSTRACT_TEXT,
                       <span class="dt">doc_names =</span> nih_sample<span class="op">$</span>APPLICATION_ID,
                       <span class="dt">ngram_window =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),
                       <span class="dt">verbose =</span> <span class="ot">FALSE</span>,
                       <span class="dt">cpus =</span> <span class="dv">2</span>)

dtm_embed &lt;-<span class="st"> </span>dtm_embed[,<span class="kw">colSums</span>(dtm_embed) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>]

<span class="co"># Project the documents into the embedding space</span>
embedding_assignments &lt;-<span class="st"> </span><span class="kw">predict</span>(embeddings, dtm_embed, <span class="dt">method =</span> <span class="st">&quot;gibbs&quot;</span>,
                                 <span class="dt">iterations =</span> <span class="dv">200</span>, <span class="dt">burnin =</span> <span class="dv">180</span>)</code></pre></div>
<p>Once you’ve embedded your documents, you effectively have a new <span class="math inline">\(\Theta\)</span>. We can use that to evaluate how well the embedding topics fit the documents as a whole by re-calculatingR-squared and coherence.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get a goodness of fit relative to the DTM</span>
embeddings<span class="op">$</span>r2_dtm &lt;-<span class="st"> </span><span class="kw">CalcTopicModelR2</span>(<span class="dt">dtm =</span> dtm_embed, 
                                      <span class="dt">phi =</span> embeddings<span class="op">$</span>phi[,<span class="kw">colnames</span>(dtm_embed)], <span class="co"># line up vocabulary</span>
                                      <span class="dt">theta =</span> embedding_assignments,
                                      <span class="dt">cpus =</span> <span class="dv">2</span>)

embeddings<span class="op">$</span>r2_dtm
<span class="co">#&gt; [1] 0.2298521</span>

<span class="co"># get coherence relative to DTM</span>
embeddings<span class="op">$</span>coherence_dtm &lt;-<span class="st"> </span><span class="kw">CalcProbCoherence</span>(<span class="dt">phi =</span> embeddings<span class="op">$</span>phi[,<span class="kw">colnames</span>(dtm_embed)], <span class="co"># line up vocabulary</span>
                                              <span class="dt">dtm =</span> dtm_embed)

<span class="kw">summary</span>(embeddings<span class="op">$</span>coherence_dtm)
<span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span>
<span class="co">#&gt; 0.02284 0.07596 0.11879 0.15447 0.20744 0.40667</span></code></pre></div>
</div>
<div id="where-to-next" class="section level2">
<h2>Where to next?</h2>
<p>Embedding research is only just beginning. I would encourage you to play with them and develop your own methods.</p>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
